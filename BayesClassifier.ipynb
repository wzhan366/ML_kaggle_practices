{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GaussianNB<br>\n",
    "GaussianNB<br>\n",
    "\t特征变量是连续变量，符合高斯分布<br>\n",
    "MultinomialNB<br>\n",
    "\t特征变量是离散变量，符合多项分布<br>\n",
    "\t\t在文档分类中特征变量体现在一个单词出现的次数，或者是单词的TF-IDF 值等<br>\n",
    "\t\tTF-IDF 值是一个统计方法，用来评估某个词语对于一个文件集或文档库中的其中一份文件的重要程度<br>\n",
    "\t\t\tTF-IDF 实际上是两个词组 Term Frequency 和 Inverse Document Frequency 的总称，两者缩写为 TF 和 IDF，分别代表了词<br>频和逆向文档频率<br>\n",
    "\tsklearn TfidfVectorizer<br>\n",
    "BernoulliNB<br>\n",
    "\t特征变量是布尔变量，符合 0/1 分布<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "# TfidfVectorizer(stop_words=stop_words, token_pattern=token_pattern)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文档 1：this is the bayes document；\n",
    "文档 2：this is the second second document；\n",
    "文档 3：and the third one；\n",
    "文档 4：is this the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vec = TfidfVectorizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    'this is the bayes document',\n",
    "    'this is the second second document',\n",
    "    'and the third one',\n",
    "    'is this the document'\n",
    "]\n",
    "tfidf_matrix = tfidf_vec.fit_transform(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('unique words:', [u'and', u'bayes', u'document', u'is', u'one', u'second', u'the', u'third', u'this'])\n"
     ]
    }
   ],
   "source": [
    "print('unique words:', tfidf_vec.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('word IDs:', {u'and': 0, u'third': 7, u'this': 8, u'is': 3, u'one': 4, u'second': 5, u'bayes': 1, u'the': 6, u'document': 2})\n"
     ]
    }
   ],
   "source": [
    "print('word IDs:', tfidf_vec.vocabulary_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('the tfidf value of words:', array([[0.        , 0.63314609, 0.40412895, 0.40412895, 0.        ,\n",
      "        0.        , 0.33040189, 0.        , 0.40412895],\n",
      "       [0.        , 0.        , 0.27230147, 0.27230147, 0.        ,\n",
      "        0.85322574, 0.22262429, 0.        , 0.27230147],\n",
      "       [0.55280532, 0.        , 0.        , 0.        , 0.55280532,\n",
      "        0.        , 0.28847675, 0.55280532, 0.        ],\n",
      "       [0.        , 0.        , 0.52210862, 0.52210862, 0.        ,\n",
      "        0.        , 0.42685801, 0.        , 0.52210862]]))\n"
     ]
    }
   ],
   "source": [
    "print('the tfidf value of words:', tfidf_matrix.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data\n",
    "https://github.com/cystanford/text_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "/home/ml/predictOptimizaitonStatus/src/product_reviews_1<br>\n",
    "    \n",
    "This folder contains annotated customer reviews of 5 products.\n",
    "\n",
    "        1. digital camera: Canon G3\n",
    "        2. digital camera: Nikon coolpix 4300\n",
    "        3. celluar phone:  Nokia 6610\n",
    "        4. mp3 player:     Creative Labs Nomad Jukebox Zen Xtra 40GB\n",
    "        5. dvd player:     Apex AD2600 Progressive-scan DVD player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Apex_AD2600_Progressive_scan_DVD player.txt', 'Canon_G3.txt', 'Creative_Labs_Nomad_Jukebox_Zen_Xtra_40GB.txt', 'Nikon_coolpix_4300.txt', 'Nokia_6610.txt', 'README.txt']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "text_directory = './product_reviews_1'\n",
    "text_list = os.listdir(text_directory)\n",
    "print text_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining stemmer, which is used to clean up the text. Normally delete some unwanted terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.stem.porter import *\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the texts\n",
    "\n",
    "1. put all them into lower words\n",
    "2. remove punctuation\n",
    "3. tokenize\n",
    "4. stem them\n",
    "5. adding label to each input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk #for english\n",
    "import string\n",
    "file_name = 'Apex_AD2600_Progressive_scan_DVD player.txt'\n",
    "label_dict = {'Apex_AD2600_Progressive_scan_DVD player.txt': 'Apex',\n",
    "             'Canon_G3.txt': 'Canon_G3',\n",
    "             'Creative_Labs_Nomad_Jukebox_Zen_Xtra_40GB.txt':'Nomad',\n",
    "             'Nikon_coolpix_4300.txt':'Nikon4300',\n",
    "             'Nokia_6610.txt':'Nokia6610'}\n",
    "data_set = []\n",
    "for file_name in text_list:\n",
    "    file_dir = text_directory+ '/' + file_name\n",
    "    with open(file_dir,'r') as f:\n",
    "        data = f.readlines()\n",
    "        for line in data:\n",
    "            if line[0] == '#':\n",
    "                l = line.replace('#', '').replace('\\n', '').replace('\\w', '')\n",
    "                lowers = l.lower()\n",
    "                no_punctuation = lowers.translate(None, string.punctuation)\n",
    "                # tokenize\n",
    "                tokens = nltk.word_tokenize(no_punctuation)\n",
    "#                 filtered = [w for w in tokens if not w in stopwords.words('english')]\n",
    "                stemmed = stem_tokens(tokens, stemmer)\n",
    "                data_set.append((' '.join(stemmed), label_dict[file_name]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming to Dataframe to simplify the sklearn implementatons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "labels = ['stemmed_review', 'label']\n",
    "df = pd.DataFrame(data_set, columns=labels)\n",
    "\n",
    "X = df['stemmed_review']\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### overview of the stats of the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count      2214\n",
       "unique        5\n",
       "top       Nomad\n",
       "freq        995\n",
       "Name: label, dtype: object"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count      2214\n",
       "unique        5\n",
       "top       Nomad\n",
       "freq        995\n",
       "Name: label, dtype: object"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Nomad        995\n",
       "Apex         395\n",
       "Canon_G3     358\n",
       "Nokia6610    280\n",
       "Nikon4300    186\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>It seems there's some sort of imbalance between labels, such as the Nikon4300 counts small portion than others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle and split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "593     i do nt need to go into exhust review of thi c...\n",
       "1023                                   thi is pretti cool\n",
       "973                                               summari\n",
       "782     you wo nt appreci the qualiti thi thing ha to ...\n",
       "1934    i am a busi user who heavili depend on mobil s...\n",
       "Name: stemmed_review, dtype: object"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "593      Canon_G3\n",
       "1023        Nomad\n",
       "973         Nomad\n",
       "782         Nomad\n",
       "1934    Nokia6610\n",
       "Name: label, dtype: object"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to download the normal stopwords file from internet and read it via python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load stop words\n",
    "\n",
    "Stop word is used to filter out the original text and calcualtion the text scores such as TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stopWords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "('input: ', (1483,))\n",
      "('label: ', (1483,))\n",
      "Test:\n",
      "('input: ', (731,))\n",
      "('label: ', (731,))\n"
     ]
    }
   ],
   "source": [
    "print('Train:')\n",
    "print('input: ', X_train.shape)\n",
    "print('label: ', y_train.shape)\n",
    "print('Test:')\n",
    "print('input: ', X_test.shape)\n",
    "print('label: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calculate TF-IDF\n",
    "\n",
    "We use this as input to bays's model, then bays model use it to calcualte the classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF 值是一个统计方法，用来评估某个词语对于一个文件集或文档库中的其中一份文件的重要程度\n",
    "\n",
    "TF-IDF 实际上是两个词组 Term Frequency 和 Inverse Document Frequency 的总称，两者缩写为 TF 和 IDF，分别代表了词\n",
    "频和逆向文档频率\n",
    "\n",
    "这里 max_df 参数用来描述单词在文档中的最高出现率。假设 max_df=0.5，代表一个单词在 50% 的文档中都出现过了，那么它只携带了非常少的信息，因此就不作为分词统计。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=stopWords, max_df=0.5)\n",
    "# train_vocabulary = tf.vocabulary_()\n",
    "X_train_unpipe = vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1483, 2355)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_unpipe.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多项式贝叶斯分类器\n",
    "from sklearn.naive_bayes import MultinomialNB  \n",
    "clf = MultinomialNB(alpha=0.0001).fit(X_train_unpipe, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.0001, 'fit_prior': True, 'class_prior': None}\n"
     ]
    }
   ],
   "source": [
    "print clf.get_params()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_test_unpipe = vectorizer.transform(X_test)\n",
    "predicted_labels=clf.predict(X_test_unpipe)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6265389876880985\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Nomad       0.67      0.40      0.50       140\n",
      "    Canon_G3       0.63      0.53      0.57       125\n",
      "   Nokia6610       0.44      0.38      0.41        63\n",
      "   Nikon4300       0.67      0.44      0.53        85\n",
      "        Apex       0.63      0.86      0.73       318\n",
      "\n",
      "   micro avg       0.63      0.63      0.63       731\n",
      "   macro avg       0.61      0.52      0.55       731\n",
      "weighted avg       0.63      0.63      0.61       731\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix \n",
    "print (metrics.accuracy_score(y_test, predicted_labels))\n",
    "confusion_matrix(y_test, predicted_labels)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "target_names = label_dict.values()\n",
    "print(classification_report(y_test, predicted_labels, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline version, be careful of the naming while changing from unpiped version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vectorizer', TfidfVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.float64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_...e,\n",
       "        vocabulary=None)), ('clf', MultinomialNB(alpha=0.001, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB  \n",
    "\n",
    "pipe = []\n",
    "# vectorizer = ('vectorizer', CountVectorizer())\n",
    "# pipe.append(vectorizer)\n",
    "\n",
    "vectorizer = ('vectorizer', TfidfVectorizer())\n",
    "pipe.append(vectorizer)\n",
    "\n",
    "p_clf =  ('clf', MultinomialNB(alpha=0.001))\n",
    "pipe.append(p_clf)\n",
    "\n",
    "\n",
    "\n",
    "pipe_clf = Pipeline(pipe)\n",
    "pipe_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = pipe_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy: ', 0.6415868673050615)\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Nomad       0.68      0.40      0.50       140\n",
      "    Canon_G3       0.66      0.56      0.61       125\n",
      "   Nokia6610       0.51      0.33      0.40        63\n",
      "   Nikon4300       0.75      0.45      0.56        85\n",
      "        Apex       0.63      0.89      0.74       318\n",
      "\n",
      "   micro avg       0.64      0.64      0.64       731\n",
      "   macro avg       0.65      0.53      0.56       731\n",
      "weighted avg       0.65      0.64      0.62       731\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix \n",
    "print ('Accuracy: ', metrics.accuracy_score(y_test, predicted_labels))\n",
    "confusion_matrix(y_test, predicted_labels)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "target_names = label_dict.values()\n",
    "print('\\n')\n",
    "print(classification_report(y_test, predicted_labels, target_names=target_names))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
